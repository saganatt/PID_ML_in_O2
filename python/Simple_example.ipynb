{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot3\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from root_utils import tree_to_pandas\n",
    "\n",
    "MODEL_PATH = \"../models/\"\n",
    "MODEL_NAME = \"Simple_example_multioutput\"\n",
    "ONNX_MODEL_PATH = \"%s/%s.onnx\" % (MODEL_PATH, MODEL_NAME)\n",
    "batch_size = 32\n",
    "data_size = 1024\n",
    "classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "\n",
    "        self.input = nn.Linear(5,15)\n",
    "        self.hidden = nn.Linear(15,10)\n",
    "        self.out = nn.Linear(10,classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.input(x))\n",
    "        x = torch.tanh(self.hidden(x))\n",
    "        x = torch.softmax(self.out(x), 0) # No softmax() if we use CrossEntropyLoss()\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "Random values with target 1 if sum of input values is greater than 2, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(data_size,5)\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.floor(x.sum(1)).long().view(-1,1)\n",
    "print(y[:5])\n",
    "\n",
    "# One hot encoding buffer that you create out of the loop and just keep reusing\n",
    "y_onehot = torch.FloatTensor(batch_size, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x,y)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.ExponentialLR(optim, gamma=0.95)\n",
    "criterion = nn.MSELoss()\n",
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "        # Converting labels to one-hot\n",
    "        y_onehot.zero_()\n",
    "        y_onehot.scatter_(1, train_label, 1)\n",
    "    #     label_np = np.zeros((train_label.shape[0], 10))\n",
    "        optim.zero_grad()\n",
    "        predict_y = model(train_x)\n",
    "        _error = criterion(predict_y, y_onehot)\n",
    "        _error.backward()\n",
    "        optim.step()\n",
    "    if epoch % 5 == 0:\n",
    "        print('epoch:{}, idx: {}, loss: {}'.format(epoch, idx, _error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.tensor([0.3,1,1.2,0.3,1])\n",
    "model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"%s/%s.torch\" % (MODEL_PATH, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving as onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model,dummy_input, ONNX_MODEL_PATH,input_names=[\"input\"],output_names=[\"out\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onnx = onnx.load(ONNX_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.checker.check_model(model_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(onnx.helper.printable_graph(model_onnx.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application with onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(ONNX_MODEL_PATH)\n",
    "outputs_meta = ort_session.get_outputs()\n",
    "outputs_meta[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.array([0.3,1,1.2,0.3,1]).astype(np.float32)\n",
    "outputs = ort_session.run(None, {'input': example})\n",
    "#outputs = torch.FloatTensor(outputs)\n",
    "#probabilities = torch.softmax(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilities\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting ROOT input files from O2 to pickle/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/home/maja/CERN_part/CERN/PID_ML_in_O2/python/data/root\"\n",
    "OUTPUT_DIR = \"/home/maja/CERN_part/CERN/PID_ML_in_O2/python/data/train_208\"\n",
    "TRAINS_REAL = [208]\n",
    "TRAINS_MC = [208]\n",
    "TRAINS_NUMBERS = [TRAINS_REAL, TRAINS_MC]\n",
    "DATASETS_REAL = [\"LHC18g4\"]\n",
    "DATASETS_MC = [\"LHC18g4\"]\n",
    "DATASETS = [DATASETS_REAL, DATASETS_MC]\n",
    "RUNS_REAL = [285064]\n",
    "RUNS_MC = [285064]\n",
    "RUNS = [RUNS_REAL, RUNS_MC]\n",
    "COUNT = 3\n",
    "\n",
    "for [train_numbers, datasets, runs, real_or_mc] in zip(TRAINS_NUMBERS, DATASETS, RUNS, (\"real\", \"mc\")):\n",
    "    for [train_number, dataset, run] in zip(train_numbers, datasets, runs):\n",
    "        for i in range(1, COUNT+1):\n",
    "            input_file = \"%s/train_%d_dataset_%s_run_%d_%03d_pidtracks%s_globaltracks.root\" %\\\n",
    "                         (INPUT_DIR, train_number, dataset, run, i, real_or_mc)\n",
    "            print(\"Input file: \", input_file)\n",
    "\n",
    "            with uproot3.open(input_file) as file:\n",
    "                for dirname in file:\n",
    "                    dirname = dirname.decode(\"utf-8\")\n",
    "                    pure_dirname = dirname.split(\";\")[0]\n",
    "                    print(\"Pure dirname: \", pure_dirname)\n",
    "                    tree_data = file[\"%s/O2pidtracks%s\" % (dirname, real_or_mc)].pandas.df()\n",
    "                    output_file = \"train_%d_pidtracks%s_globaltracks_%s\" % (train_number, real_or_mc, pure_dirname)\n",
    "                    print(\"Output file: \", output_file)\n",
    "                    #tree_data.to_csv(\"%s.csv\" % output_file, sep=\",\")\n",
    "                    tree_data.to_pickle(\"%s/pkl/%s.pkl\" % (OUTPUT_DIR, output_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
